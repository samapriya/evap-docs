{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Reservoir Evaporation API Documentation","text":"<p>Beta Status: This website is currently under development and may have some limitations or issues. Evaporation data should be considered provisional and not used for operational support or decision making. A production version of this API and database is scheduled to launch late summer 2025. </p>"},{"location":"#overview","title":"Overview","text":"<p>The BOR Reservoir Evaporation API provides public access to historical, and near-real-time daily evaporation estimates for 247 Bureau of Reclamation reservoirs located across the 17 western states. Reservoir evaporation estimates are generated using the Texas A&amp;M Daily Lake Evaporation Model (DLEM) (Zhao and Gao, 2019; Zhao et al., 2024). </p> <p>This API delivers high-quality data records of evaporation rates and volumes for major reservoirs. The evaporation estimates incorporate meteorological forcing data and reservoir storage information to provide the best available estimates of reservoir evaporation.</p>"},{"location":"#about-this-project","title":"About This Project","text":"<p>Open-water evaporation represents a complex physical process that influences both the water and energy budgets of a lake or reservoir. Though open-water evaporation is critical for water quality, water distribution, and legal accounting, developing reliable estimates is challenging.</p> <p>Historically, water management agencies such as the Bureau of Reclamation (Reclamation) have relied on evaporation estimates from Class A pans for water budget and accounting purposes. While simple and relatively inexpensive to maintain, Class A pans and the associated evaporation estimates have known biases relative to more advanced estimation techniques. This bias, which can depend on water body characteristics like depth and volume, is often attributed to a lack of heat storage in Class A pans relative to real water bodies.</p> <p>This project developed a daily reservoir evaporation database which can be freely accessed and visualized by water managers and stakeholders. This database contains historical and near real-time, high quality data records of evaporation rates and volumes for major reservoirs.</p> <p>Data can be accessed interactively via the web interface at: - https://dri-apps.earthengine.app/view/bor-reservoir-evaporation</p> <p>Or programmatically through the API at: - https://operevap.dri.edu/docs</p> <p>See the Getting Started section for help with the API.</p>"},{"location":"#collaborative-development","title":"Collaborative Development","text":"<p>This dataset and API was collaboratively developed by the Reclamation, Desert Research Institute, Texas A&amp;M University, Virginia Tech University, and NASA. </p> <p> <p></p> <p></p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>Data and information provided through this application are part of an active research project and should be considered provisional and subject to change. A production version of this API and database is scheduled to launch late summer 2025. Users should perform thorough review prior to operational application and decision making.</p>"},{"location":"#funding-and-support","title":"Funding and Support","text":"<p>This project received funding from the BOR Research and Development Office (R20AC00008) and NASA Water Resources Program (80NSSC22K0933). Additional support provided by the USGS, USACE, the Texas Water Development Board, and the Oregon Water Resources Department.</p>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li>Abatzoglou, J. T. (2013), Development of gridded surface meteorological data for ecological applications and modelling. Int. J. Climatol., 33: 121\u2013131.</li> <li>De Pondeca, M. S. F. V., and Coauthors, 2011: The Real-Time Mesoscale Analysis at NOAA's National Centers for Environmental Prediction: Current status and development. Wea. Forecasting, 26, 593\u2013612, https://doi.org/10.1175/WAF-D-10-05037.1.</li> <li>Tanny, J., and Coauthors, 2008: Evaporation from a small water reservoir: Direct measurements and estimates. J. Hydrology, 351, 218-229.</li> <li>Zhao, G., and H. Gao, 2019: Estimating reservoir evaporation losses for the united states: Fusing remote sensing and modeling approaches. Remote Sensing of Environment, 226, 109\u2013124.</li> <li>Zhao, B., Huntington, J., Pearson, C., Zhao, G., Ott, T., Zhu, J., ... &amp; Gao, H. (2024). Developing a general Daily Lake Evaporation Model and demonstrating its application in the state of Texas. Water Resources Research, 60(3), e2023WR036181.</li> </ul>"},{"location":"appendix/","title":"Appendix","text":""},{"location":"appendix/#available-variables","title":"Available Variables","text":""},{"location":"appendix/#reservoir-variables","title":"Reservoir Variables","text":"<p>The following variables are available for reservoirs across different datasets:</p>"},{"location":"appendix/#rtma-dataset-weather-variables","title":"RTMA Dataset (Weather Variables)","text":"Variable Description Units (Metric) Units (English) pr Precipitation mm in tmmx_c Maximum temperature \u00b0C \u00b0F tmmn_c Minimum temperature \u00b0C \u00b0F vpd_kpa Vapor pressure deficit kPa kPa vs2m Wind speed at 2 meters m/s mph srad Solar radiation W/m\u00b2 W/m\u00b2 th Specific humidity kg/kg kg/kg sph_kgkg Specific humidity kg/kg kg/kg pres_pa Air pressure Pa Pa <p>*Pre-2016 weather data is derived from the hybrid RTMA-gridMET dataset. See the Methods section for additional details.</p>"},{"location":"appendix/#res-dataset-reservoir-physical-variables","title":"RES Dataset (Reservoir Physical Variables)","text":"Variable Description Units (Metric) Units (English) stage_m Water surface elevation m ft area_m2 Surface area m\u00b2 acre depth_m Average depth m ft"},{"location":"appendix/#lem-dataset-dlem-evaporation-and-heat-stoarge-variables","title":"LEM Dataset (DLEM Evaporation and Heat Stoarge Variables)","text":"Variable Description Units (Metric) Units (English) E_hs Heat storage evaporation mm/day in/day E_nhs No heat storage evaporation mm/day in/day Rn Net radiation W/m\u00b2 W/m\u00b2 Tw Water temperature \u00b0C \u00b0F Tw0 Surface water temperature \u00b0C \u00b0F Fetch Fetch m ft Lerr Latent heat of evaporation W/m\u00b2 W/m\u00b2"},{"location":"appendix/#nete-volume-calcs-dataset-net-evaporation-and-evaporation-volume-variables","title":"NETE-VOLUME-CALCS Dataset (Net Evaporation and Evaporation Volume Variables)","text":"Variable Description Units (Metric) Units (English) NetE Net evaporation mm/day in/day E_volume Evaporation volume m\u00b3 acre-ft NetE_volume Net evaporation volume m\u00b3 acre-ft"},{"location":"appendix/#http-status-codes","title":"HTTP Status Codes","text":"<p>The API may return the following HTTP status codes:</p> Status Code Description 200 Successful response 401 Unauthorized (invalid or missing API key) 404 Not found (resource not found) 422 Validation error (invalid parameters) 500 Server error"},{"location":"appendix/#data-quality-flags","title":"Data Quality Flags","text":"<p>The API may include quality flags in the response data. These flags indicate the quality or reliability of the data:</p> Flag Description 0 Good data 1 Suspect data (automatically flagged) 2 Suspect data (manually flagged) 3 Missing data (gap filled) 9 Missing data (not filled)"},{"location":"appendix/#file-formats","title":"File Formats","text":""},{"location":"appendix/#shapefile-components","title":"Shapefile Components","text":"<p>When uploading shapefiles to filter reservoirs or stations by location, the following files are required:</p> File Extension Description .shp The main file that stores the geometry .dbf The database file that stores the attributes .prj The projection file that stores the coordinate system .shx The index file that stores the index of the geometry"},{"location":"appendix/#glossary","title":"Glossary","text":"Term Definition Evaporation The process by which water changes from a liquid to a gas or vapor Net Evaporation The difference between evaporation and precipitation (evaporation minus precipitation) Reservoir A large natural or artificial lake used as a source of water supply Timeseries A series of data points indexed in time order Metadata Data that provides information about other data API Application Programming Interface REST Representational State Transfer, an architectural style for APIs JSON JavaScript Object Notation, a lightweight data-interchange format CSV Comma-Separated Values, a text file format that uses commas to separate values Dataset A collection of data Variable A measurable property Units The standard of measurement Metric The International System of Units (SI) English The US Customary Units Quality Flag An indicator of the quality or reliability of the data Shapefile A geospatial vector data format for geographic information system software"},{"location":"changelog/","title":"Changelog","text":""},{"location":"examples/","title":"Examples","text":"<p>This section provides example code snippets for common tasks using the Reservoir Evaporation API.</p> <p></p>"},{"location":"examples/#setup","title":"Setup","text":""},{"location":"examples/#set-up-a-virtual-environment","title":"Set up a virtual environment","text":"<pre><code>python3 -m venv venv\nsource venv/bin/activate\npip install geopandas\npip install matplotlib\npip install pandas\npip install requests\n</code></pre> PythoncURL <pre><code>import requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\n\n# API configuration\nAPI_KEY = \"your_api_key_here\"\nBASE_URL = \"https://operevap.dri.edu/\"  # Replace with the actual API base URL\nHEADERS = {\n    \"api-key\": API_KEY\n}\n</code></pre> <pre><code># Store your API key and base URL for reuse\nAPI_KEY=\"your_api_key_here\"\nBASE_URL=\"https://api-url.example\"  # Replace with the actual API base URL\n</code></pre>"},{"location":"examples/#get-available-reservoirs","title":"Get Available Reservoirs","text":"PythoncURL <pre><code>def get_reservoirs():\n    \"\"\"Get a list of available reservoirs.\"\"\"\n    url = f\"{BASE_URL}/info/list_RES_NAMES\"\n    response = requests.post(url, headers=HEADERS)\n\n    if response.status_code == 200 and \"RES_NAMES\" in response.json().keys():\n        return response.json()[\"RES_NAMES\"]\n    else:\n        print(f\"Error: {response.status_code}\")\n        return None\n\n# Example usage\nreservoirs = get_reservoirs()\nif reservoirs:\n    print(f\"Number of reservoirs: {len(reservoirs)}\")\n    print(\"First 5 reservoirs:\")\n    for res in reservoirs[:5]:\n        print(f\"- {res}\")\n</code></pre> <pre><code># Get a list of available reservoirs\ncurl -X POST \"${BASE_URL}/info/list_RES_NAMES\" \\\n     -H \"api-key: ${API_KEY}\" \\\n     -H \"Content-Type: application/json\"\n</code></pre>"},{"location":"examples/#get-reservoir-metadata","title":"Get Reservoir Metadata","text":"PythoncURL <pre><code>def get_reservoir_metadata(reservoir_names):\n    \"\"\"Get metadata for specified reservoirs.\"\"\"\n    url = f\"{BASE_URL}/metadata/reservoirs\"\n    params = {\n        \"RES_NAMES\": \",\".join(reservoir_names),\n        \"output_format\": \"json\"\n    }\n\n    response = requests.get(url, headers=HEADERS, params=params)\n\n    if response.status_code == 200:\n        return response.json()\n    else:\n        print(f\"Error: {response.status_code}\")\n        return None\n\n# Example usage\nreservoir_metadata = get_reservoir_metadata([\"LAKE ALICE\", \"LAKE ESTES\"])\nfor res in reservoir_metadata:\n    print(f\"Reservoir: {res['RES_NAME']}\")\n    print(f\"  Latitude: {res['LAT']}\")\n    print(f\"  Longitude: {res['LON']}\")\n    print(f\"  State: {res['Shape_Area']}\")\n</code></pre> <pre><code># Get metadata for specified reservoirs\ncurl -X GET \"${BASE_URL}/metadata/reservoirs?RES_NAMES=LAKE%20ALICE,LAKE%20ESTES&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"examples/#get-timeseries-data-for-a-reservoir","title":"Get Timeseries Data for a Reservoir","text":"PythoncURL <pre><code>def get_reservoir_timeseries(reservoir_name, start_date, end_date):\n    \"\"\"Get timeseries data for a reservoir.\"\"\"\n    url = f\"{BASE_URL}/timeseries/daily/reservoirs/daterange\"\n    params = {\n        \"RES_NAMES\": reservoir_name,\n        \"datasets\": \"nete-volume-calcs\",\n        \"variables\": \"NetE,E_volume\",\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"units\": \"metric\",\n        \"output_format\": \"json\"\n    }\n\n    response = requests.get(url, headers=HEADERS, params=params)\n\n    if response.status_code == 200:\n        return response.json()\n    else:\n        print(f\"Error: {response.status_code}\")\n        return None\n\n# Example usage\ndata = get_reservoir_timeseries(\"LAKE ALICE\", \"2020-01-01\", \"2020-12-31\")\n\n# Convert to pandas DataFrame for analysis\ndf = pd.DataFrame(data)\ndf['start_date'] = pd.to_datetime(df['start_date'])\ndf.set_index('start_date', inplace=True)\n\n# Plot the data\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df['NetE (mm)'], label='Net Evaporation')\nplt.title('Net Evaporation for LAKE ALICE (2020)')\nplt.xlabel('Date')\nplt.ylabel('Net Evaporation (mm/day)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code># Get timeseries data for a reservoir\ncurl -X GET \"${BASE_URL}/timeseries/daily/reservoirs/daterange?RES_NAMES=LAKE%20ALICE&amp;datasets=nete-volume-calcs&amp;variables=NetE,E_volume&amp;start_date=2020-01-01&amp;end_date=2020-12-31&amp;units=metric&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"examples/#compare-evaporation-between-multiple-reservoirs","title":"Compare Evaporation Between Multiple Reservoirs","text":"PythoncURL <pre><code>def compare_reservoirs(reservoir_names, start_date, end_date, variable=\"NetE\"):\n    \"\"\"Compare a variable between multiple reservoirs.\"\"\"\n    url = f\"{BASE_URL}/timeseries/daily/reservoirs/daterange\"\n    params = {\n        \"RES_NAMES\": \",\".join(reservoir_names),\n        \"datasets\": \"nete-volume-calcs\",\n        \"variables\": variable,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"units\": \"metric\",\n        \"output_format\": \"json\"\n    }\n\n    response = requests.get(url, headers=HEADERS, params=params)\n\n    if response.status_code == 200:\n        return response.json()\n    else:\n        print(f\"Error: {response.status_code}\")\n        return None\n\n# Example usage\nreservoirs_to_compare = [\"LAKE ALICE\", \"LAKE ESTES\", \"RUEDI RESERVOIR\"]\ncomparison_data = compare_reservoirs(reservoirs_to_compare, \"2020-06-01\", \"2020-08-31\")\n\n# Process and visualize data\ndf = pd.DataFrame(comparison_data)\ndf['start_date'] = pd.to_datetime(df['start_date'])\n\n# Pivot the data to have one column per reservoir\npivot_df = df.pivot_table(index='start_date', columns='RES_NAME', values='NetE (mm)')\n\n# Plot the data\nplt.figure(figsize=(12, 6))\nfor reservoir in reservoirs_to_compare:\n    plt.plot(pivot_df.index, pivot_df[reservoir], label=reservoir)\n\nplt.title('Net Evaporation Comparison (Summer 2020)')\nplt.xlabel('Date')\nplt.ylabel('Net Evaporation (mm/day)')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code># Compare evaporation between multiple reservoirs\ncurl -X GET \"${BASE_URL}/timeseries/daily/reservoirs/daterange?RES_NAMES=LAKE%20ALICE,LAKE%20ESTES,RUEDI%20RESERVOIR&amp;datasets=nete-volume-calcs&amp;variables=NetE&amp;start_date=2020-06-01&amp;end_date=2020-08-31&amp;units=metric&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"examples/#calculate-monthly-evaporation-volumes","title":"Calculate Monthly Evaporation Volumes","text":"PythoncURL <pre><code>def get_monthly_evaporation_volumes(reservoir_name, year):\n    \"\"\"Calculate monthly evaporation volumes for a reservoir.\"\"\"\n    start_date = f\"{year}-01-01\"\n    end_date = f\"{year}-12-31\"\n\n    url = f\"{BASE_URL}/timeseries/daily/reservoirs/daterange\"\n    params = {\n        \"RES_NAMES\": reservoir_name,\n        \"datasets\": \"nete-volume-calcs\",\n        \"variables\": \"E_volume\",\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"units\": \"metric\",\n        \"output_format\": \"json\"\n    }\n\n    response = requests.get(url, headers=HEADERS, params=params)\n\n    if response.status_code == 200:\n        data = response.json()\n        df = pd.DataFrame(data)\n        df['start_date'] = pd.to_datetime(df['start_date'])\n        df.set_index('start_date', inplace=True)\n\n        # Calculate monthly totals\n        monthly_data = df.resample('MS').sum()\n        monthly_data.index = monthly_data.index.strftime('%b')\n\n        return monthly_data\n    else:\n        print(f\"Error: {response.status_code}\")\n        return None\n\n# Example usage\nmonthly_volumes = get_monthly_evaporation_volumes(\"LAKE MEAD\", 2020)\n\n# Plot monthly volumes\nplt.figure(figsize=(12, 6))\nplt.bar(monthly_volumes.index, monthly_volumes['E_volume (m3)'])\nplt.title('Monthly Evaporation Volumes for LAKE MEAD (2020)')\nplt.xlabel('Month')\nplt.ylabel('Evaporation Volume (cubic meters)')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code># Get monthly evaporation volumes data\ncurl -X GET \"${BASE_URL}/timeseries/daily/reservoirs/daterange?RES_NAMES=LAKE%20MEAD&amp;datasets=nete-volume-calcs&amp;variables=E_volume&amp;start_date=2020-01-01&amp;end_date=2020-12-31&amp;units=metric&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"examples/#retrieve-and-analyze-weather-data-for-a-reservoir","title":"Retrieve and Analyze Weather Data for a Reservoir","text":"PythoncURL <pre><code>def get_weather_data(reservoir_name, start_date, end_date):\n    \"\"\"Get weather data for a reservoir.\"\"\"\n    url = f\"{BASE_URL}/timeseries/daily/reservoirs/daterange\"\n    params = {\n        \"RES_NAMES\": reservoir_name,\n        \"datasets\": \"rtma\",\n        \"variables\": \"pr,tmmx_c,tmmn_c,vpd_kpa,srad\",\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"units\": \"metric\",\n        \"output_format\": \"json\"\n    }\n\n    response = requests.get(url, headers=HEADERS, params=params)\n\n    if response.status_code == 200:\n        return response.json()\n    else:\n        print(f\"Error: {response.status_code}\")\n        return None\n\n# Example usage\nweather_data = get_weather_data(\"LAKE POWELL\", \"2020-06-01\", \"2020-08-31\")\ndf = pd.DataFrame(weather_data)\ndf['start_date'] = pd.to_datetime(df['start_date'])\ndf.set_index('start_date', inplace=True)\n\n# Calculate monthly averages\nmonthly_weather = df.resample('MS').agg({\n    'pr (mm)': 'sum',\n    'tmmx_c (degC)': 'mean',\n    'tmmn_c (degC)': 'mean',\n    'vpd_kpa (kpa)': 'mean',\n    'srad (wm2)': 'mean'\n})\n\nprint(\"Monthly Weather Summary for LAKE POWELL (Summer 2020):\")\nprint(f\"Month | Total Precip (mm) | Avg Max Temp (\u00b0C) | Avg Min Temp (\u00b0C)\")\nfor idx, row in monthly_weather.iterrows():\n    month = idx.strftime('%b %Y')\n    print(f\"{month} | {row['pr (mm)']:.1f} | {row['tmmx_c (degC)']:.1f} | {row['tmmn_c (degC)']:.1f}\")\n</code></pre> <pre><code># Get weather data for a reservoir\ncurl -X GET \"${BASE_URL}/timeseries/daily/reservoirs/daterange?RES_NAMES=LAKE%20POWELL&amp;datasets=rtma&amp;variables=pr,tmmx_c,tmmn_c,vpd_kpa,srad&amp;start_date=2020-06-01&amp;end_date=2020-08-31&amp;units=metric&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"examples/#working-with-multiple-datasets","title":"Working with Multiple Datasets","text":""},{"location":"examples/#combine-evaporation-and-weather-data","title":"Combine Evaporation and Weather Data","text":"PythoncURL <pre><code>def combine_datasets(reservoir_name, start_date, end_date):\n    \"\"\"Combine evaporation and weather data for analysis.\"\"\"\n    # Get evaporation data\n    evap_url = f\"{BASE_URL}/timeseries/daily/reservoirs/daterange\"\n    evap_params = {\n        \"RES_NAMES\": reservoir_name,\n        \"datasets\": \"nete-volume-calcs\",\n        \"variables\": \"NetE\",\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"units\": \"metric\",\n        \"output_format\": \"json\"\n    }\n\n    evap_response = requests.get(evap_url, headers=HEADERS, params=evap_params)\n\n    # Get weather data\n    weather_url = f\"{BASE_URL}/timeseries/daily/reservoirs/daterange\"\n    weather_params = {\n        \"RES_NAMES\": reservoir_name,\n        \"datasets\": \"rtma\",\n        \"variables\": \"tmmx_c\",\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"units\": \"metric\",\n        \"output_format\": \"json\"\n    }\n\n    weather_response = requests.get(weather_url, headers=HEADERS, params=weather_params)\n\n    if evap_response.status_code == 200 and weather_response.status_code == 200:\n        evap_data = pd.DataFrame(evap_response.json())\n        weather_data = pd.DataFrame(weather_response.json())\n\n        # Prepare data\n        evap_data['date'] = pd.to_datetime(evap_data['start_date']).dt.strftime('%Y-%m-%d')\n        evap_data.set_index(['date', 'RES_NAME'], inplace=True)\n\n        weather_data['date'] = pd.to_datetime(weather_data['start_date']).dt.strftime('%Y-%m-%d')\n        weather_data.set_index(['date', 'RES_NAME'], inplace=True)\n\n        # Merge datasets\n        combined_data = pd.merge(evap_data, weather_data, left_index=True, right_index=True)\n\n        return combined_data\n    else:\n        print(f\"Error: {evap_response.status_code} or {weather_response.status_code}\")\n        return None\n\n# Example usage\ncombined_data = combine_datasets(\"LAKE MEAD\", \"2020-06-01\", \"2020-08-31\")\ncombined_data.reset_index(inplace=True)\ncombined_data = combined_data[['tmmx_c (degC)', 'NetE (mm)']]\n\n# Calculate correlation between variables\ncorrelation = combined_data.corr()\nprint(\"Correlation Matrix:\")\nprint(correlation)\n\n# Plot relationship between temperature and evaporation\nplt.figure(figsize=(10, 6))\nplt.scatter(combined_data['tmmx_c (degC)'], combined_data['NetE (mm)'])\nplt.title('Relationship Between Maximum Temperature and Net Evaporation')\nplt.xlabel('Maximum Temperature (\u00b0C)')\nplt.ylabel('Net Evaporation (mm/day)')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code># Get evaporation data\ncurl -X GET \"${BASE_URL}/timeseries/daily/reservoirs/daterange?RES_NAMES=LAKE%20MEAD&amp;datasets=nete-volume-calcs&amp;variables=NetE,E_volume&amp;start_date=2020-06-01&amp;end_date=2020-08-31&amp;units=metric&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\" &gt; evap_data.json\n\n# Get weather data\ncurl -X GET \"${BASE_URL}/timeseries/daily/reservoirs/daterange?RES_NAMES=LAKE%20MEAD&amp;datasets=rtma&amp;variables=pr,tmmx_c,tmmn_c,vpd_kpa,srad&amp;start_date=2020-06-01&amp;end_date=2020-08-31&amp;units=metric&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\" &gt; weather_data.json\n\n# Note: Further processing would need to be done with tools like jq or imported into another tool\n</code></pre>"},{"location":"examples/#exporting-data-to-csv","title":"Exporting Data to CSV","text":"PythoncURL <pre><code>def export_data_to_csv(reservoir_names, start_date, end_date, output_file):\n    \"\"\"Export data for multiple reservoirs to CSV.\"\"\"\n    url = f\"{BASE_URL}/timeseries/daily/reservoirs/daterange\"\n    params = {\n        \"RES_NAMES\": \",\".join(reservoir_names),\n        \"datasets\": \"nete-volume-calcs\",\n        \"variables\": \"NetE,E_volume\",\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"units\": \"metric\",\n        \"output_format\": \"json\"\n    }\n\n    response = requests.get(url, headers=HEADERS, params=params)\n\n    if response.status_code == 200:\n        data = response.json()\n        df = pd.DataFrame(data)\n        df.to_csv(output_file, index=False)\n        print(f\"Data exported to {output_file}\")\n        return True\n    else:\n        print(f\"Error: {response.status_code}\")\n        return False\n\n# Example usage\nreservoir_list = [\"LAKE POWELL\", \"LAKE MEAD\", \"FLAMING GORGE RESERVOIR\"]\nexport_data_to_csv(reservoir_list, \"2020-01-01\", \"2020-12-31\", \"colorado_river_basin_evaporation_2020.csv\")\n</code></pre> <pre><code># Export data to a file\ncurl -X GET \"${BASE_URL}/timeseries/daily/reservoirs/daterange?RES_NAMES=LAKE%20POWELL,LAKE%20MEAD,FLAMING%20GORGE%20RESERVOIR&amp;datasets=nete-volume-calcs&amp;variables=NetE,E_volume&amp;start_date=2020-01-01&amp;end_date=2020-12-31&amp;units=metric&amp;output_format=csv\" \\\n     -H \"api-key: ${API_KEY}\" &gt; colorado_river_basin_evaporation_2020.csv\n</code></pre> <p>These examples demonstrate common use cases for the Reservoir Evaporation API and provide a starting point for your own analysis. You can modify and extend these examples to suit your specific needs.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#how-can-i-access-the-data","title":"How can I access the data?","text":"<p>There are three ways to access the data:</p> <ol> <li> <p>The web application for data visualization    https://dri-apps.earthengine.app/view/bor-reservoir-evaporation</p> </li> <li> <p>The API Playground for data inspection    https://operevap.dri.edu</p> </li> </ol>"},{"location":"faq/#how-do-i-cite-this-dataset","title":"How do I cite this dataset?","text":""},{"location":"faq/#data-citation","title":"Data citation:","text":"<p>Beta Status: This website is currently under development and may have some limitations or issues. Evaporation data should be considered provisional and not used for operational support or decision making. A production version of this API and database is scheduled to launch late summer 2025.</p>"},{"location":"faq/#related-dlem-scientific-publications","title":"Related DLEM Scientific Publications:","text":"<p>Zhao, G., &amp; Gao, H. (2019). Estimating reservoir evaporation losses for the United States: Fusing remote sensing and modeling approaches. Remote Sensing of Environment, 226, 109-124.</p> <p>Zhao, B., Huntington, J., Pearson, C., Zhao, G., Ott, T., Zhu, J., ... &amp; Gao, H. (2024). Developing a general Daily Lake Evaporation Model and demonstrating its application in the state of Texas. Water Resources Research, 60(3), e2023WR036181.</p>"},{"location":"faq/#how-often-is-data-updated","title":"How often is data updated?","text":"<p>The reservoir evaporation database is updated daily; however, the gridded weather forcing data is produced with a 2-day latency leading to a 3-day lag between today and current estimates. We are currently exploring alternative weather datasets to reduce the lag in evaporation estimates.</p>"},{"location":"faq/#my-dataset-shows-negative-evaporation-rates","title":"My dataset shows negative evaporation rates?","text":"<p>Negative evaporation (or condensation) occurs when the temperature of the water body falls below the dew point of the air above (cold water; moist air). Many models neglect to capture this process, but at certain reservoirs during certain time periods, condensation can account for a substantial portion of a reservoir's water balance. DLEM showed good agreement with negative evaporation estimates from Eddy Covariance at Lake Limestone, TX and other locations.</p> <p> <p></p> <p></p>"},{"location":"faq/#data-i-downloaded-previously-changed","title":"Data I downloaded previously changed?","text":"<p>The input weather dataset used by DLEM provides provisional estimates for ~2 months to allow for review and updates. The reservoir evaporation database updates the last 2-months of data each day to incorporate changes in the forcing data and gather in-situ data that may not have been available during the initial model run. Early, Provisional, and Final data flags are included with each daily estimate for tracking.</p>"},{"location":"faq/#my-reservoir-shows-static-values-for-area-elevation-and-volume","title":"My reservoir shows static values for area, elevation, and volume?","text":"<p>Area, elevation, and volume information is not available in real-time for all reservoirs. For static locations, we assume area, depth, and volume based on conditions at full storage. If you have area, elevation, and volume information you'd like incorporated, please contact our team.</p>"},{"location":"faq/#reservoir-area-elevation-and-volume-values-disagree-with-internal-or-operational-estimates-from-my-agency","title":"Reservoir area, elevation, and volume values disagree with internal or operational estimates from my agency?","text":"<p>Reservoir AEV information is collated from numerous online databases and resources (see sources documentation here). In many cases, reservoir Area-Elevation-Volume information is updated based and updated bathymetric survey data that may not align with historical estimates. For consistency amongst each evaporation timeseries, the XXXX workflow uses elevation combined with the latest available AEV curves to estimate volume and area. Stakeholders should use the best available judgement when applying estimates and may choose to overwrite area, elevation, and volume information as needed.</p> <p>Note</p> <p>Depths greater than 20 m are treated the same within the DLEM heat storage routine; however, depths less than 20 meters are considered and will influence evaporation estimates.</p>"},{"location":"faq/#how-can-i-obtain-an-api-key","title":"How can I obtain an API key?","text":"<ol> <li>Go to https://operevap.dri.edu</li> <li>Click the down arrow by auth/request_key</li> <li>Click on \"Try it out\"</li> <li>Fill out the form</li> <li>Click 'Execute'</li> <li>Your API key will be issued within 24 hours via email</li> </ol>"},{"location":"faq/#what-do-the-data-flags-mean","title":"What do the data flags mean?","text":"<p>If you run a data request with the option <code>also_return=qflag</code>, you will see a qflag property in the return. This property is data quality flag:</p> <ul> <li>O: raw data value from data sites</li> <li>M: data was not available for this date and is filled with previous date data value</li> <li>S: data point is static</li> <li>R: removed outlier data</li> </ul>"},{"location":"license/","title":"License","text":"<p> <pre><code>Apache License\nVersion 2.0, January 2004\nhttp://www.apache.org/licenses/\n</code></pre> </p> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>\u00a9 You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright {2025} {Samapriya Roy}</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"methods/","title":"Methods","text":"<p>The Reservoir Evaporation API integrates daily gridded weather and reservoir property data into the Penman-based Daily Lake Evaporation Model, enabling the simulation of daily evaporation rates at each relevant reservoir. These rates are then combined with surface area estimates and precipitation data to produce net evaporation rates, volumes, and associated outputs. The integrated data is stored in a geodatabase for secure storage and easy access via a web interface and API.</p> <p> <p></p> <p></p> <p>Figure: Conceptual diagram of reservoir evaporation processing pipeline and geodatabase storage and access end points.</p>"},{"location":"methods/#daily-lake-evaporation-model","title":"Daily Lake Evaporation Model","text":"<p>Reservoir evaporation estimates produced by this project are based on a daily version of Texas A&amp;M Lake Evaporation Model (DLEM) developed by Zhao and Gao (2019) and Zhao et al. (2024). DLEM uses a Penman combination equation along with reservoir fetch and heat storage effects represented. DLEM requires input solar radiation, wind speed and direction, air temperature, vapor pressure deficit, and reservoir area and depth data. Meteorological forcing data is adjusted to over water conditions using wind functions developed by McJannet (2012).</p> <p>Reservoir heat storage within DLEM is simulated using an equilibrium temperature approach where water column temperature at the current timestep is calculated from water temperature at the current timestep, equilibrium temperature, and a lag time. The equilibrium temperature is defined as the water temperature at which there is no heat exchange between air and water. The lag time is a function of reservoir depth. Inclusion of heat storage adjustments improves evaporation estimates in large, deep-water bodies where a significant amount of radiative energy in the spring goes towards warming the water body rather than fueling evaporation. Conversely, in the fall when the water body is warm and air temperatures are cool, heat storage can increase evaporation rates. Daily evaporation rate estimates are combined with reservoir surface area information to estimate volumetric losses.</p>"},{"location":"methods/#reservoir-depth-area-and-volumes","title":"Reservoir Depth, Area, and Volumes","text":"<p>Historical and real-time reservoir area, depth, and storage volume information for each of the 247 Reclamation reservoirs was obtained through Python-based queries to four separate data servers: Reclamation\u2019s Hydromet, United States Geological Survey (USGS) National Water Information System (NWIS), Reclamation Information Sharing Environment (RISE), and the California Data Exchange Center (CDEC). Across all online databases, water surface elevation data was the most consistently accessible variable and had significantly fewer quality issues than either reservoir surface area or reservoir volume. Consequently, water surface elevation data was retrieved and used to estimate surface area and volume. After retrieval, an inter-quartile range (IQR) outlier method with a fence threshold of 1.5 (used to scale the IQR) was applied to the historical elevation time series to remove outlier values.</p> <p>After filtering, a forward fill method was used to fill all missing data (i.e., records that were originally missing and/or records that were filtered out with the IQR method). For example, when no data were available for a select period, the last observed elevation value was assumed for the entire missing period until quality data observations resumed. Surface area and volume data were calculated using available site-specific area-capacity curves gathered from RISE. When multiple area-capacity curves were available for a single reservoir, the most recently developed curve was used to reconstruct surface area and volume values for all dates in the historical record. Average reservoir depth, a required input to DLEM, was then calculated as the ratio of daily reservoir volume to daily reservoir surface area for each day in the historical period. Average reservoir depth values before consistent elevation data are based on maximum reservoir capacity characteristics (e.g., volume and surface area). In the absence of readily accessible data or historical area-elevation-volume curves, a static reservoir depth was assumed based on full pool conditions.</p>"},{"location":"methods/#weather-data","title":"Weather Data","text":""},{"location":"methods/#real-time-mesoscale-analysis","title":"Real-time Mesoscale Analysis","text":"<p>Daily near surface weather conditions were extracted from the RTMA dataset stored within Google\u2019s Earth Engine Data Catalog. RTMA provides hourly estimates of near surface windspeed and direction, temperature, specific humidity, and pressure at 2.5 km resolution (De Pondeca et., al. 2011). Hourly data were aggregated to daily timesteps prior to extraction. Daily RTMA data aggregation start and end at 6 UTC to represent central standard time midnight to midnight day.  Data extraction workflows utilized the RTMA water mask combined with reservoir maximum areal extent information to extract meteorological data most representative of average over water conditions for each reservoir (Figure 3). Cell selection for smaller reservoirs with no water masked values was based on minimum cell elevation for all cells intersecting the reservoir\u2019s maximum extent.</p>"},{"location":"methods/#pre-2016-data","title":"Pre-2016 Data","text":"<p>The Google Earth Engine RTMA dataset begins in 2011, however, data coverage prior to 2016 includes numerous gaps and missing values. To extend the evaporation estimate record back to 1980 we incorporated information from the gridMET climate dataset (Abatzoglou, 2013). All evaporation estimates from 2016 forward use RTMA directly, however, estimates prior to 2016 utilize a hybrid meteorological dataset based on direct comparisons of RTMA with gridMET. gridMET provides near surface meteorological estimates at 4km resolution from 1979\u2012present, however, gridMET does not incorporate buoy-based station data or conditioning for overwater conditions and can overestimate aridity in irrigated and, or wet environments (Huntington, 2011; Abatzoglou, 2013).</p> <p>Bias correction factors were developed by comparing average monthly conditions from RTMA with gridMET for each 4km gridMET grid cell. Prior to comparison RTMA was resampled to the 4 km gridMET grid using a mean reducer. Monthly ratios of RTMA to gridMET were developed based on 2016-2021 data for windspeed, specific humidity, and solar radiation. Temperature correction factors were developed by subtracting gridMET monthly average minimum and maximum temperature from RTMA. Bias correction factors were then applied to each individual forcing variable on a monthly basis prior to model input. Similar bias correction workflows have been utilized to estimate reference evapotranspiration from gridMET based on comparisons between agricultural weather stations and gridMET (Melton et al., 2021, Volk et al., 2021).</p>"},{"location":"methods/#references","title":"References","text":"<pre><code>Abatzoglou, John T. \"Development of gridded surface meteorological data for ecological applications and modelling.\"\u202fInternational Journal of Climatology\u202f33.1 (2013): 121-131.\n\nDe Pondeca, Manuel SFV, et al. \"The real-time mesoscale analysis at NOAA\u2019s National Centers for Environmental Prediction: current status and development.\"\u202fWeather and Forecasting\u202f26.5 (2011): 593-612.\n\nMcJannet, David L., Ian T. Webster, and Freeman J. Cook. \"An area-dependent wind function for estimating open water evaporation using land-based meteorological data.\"\u202fEnvironmental modelling &amp; software\u202f31 (2012): 76-83.\n\nMelton, Forrest S., et al. \"Openet: Filling a critical data gap in water management for the western united states.\"\u202fJAWRA Journal of the American Water Resources Association\u202f(2021).\n\nVolk, John, et al. \"OpenET Satellite-based ET Intercomparisons with Ground-based Measurements: Phase II Results.\"\u202fAGU Fall Meeting Abstracts. Vol. 2021. 2021.\n\nZhao, Gang, and Huilin Gao. \"Estimating reservoir evaporation losses for the United States: Fusing remote sensing and modeling approaches.\"\u202fRemote Sensing of Environment\u202f226 (2019): 109-124.\n\nZhao, Bingjie, et al. \"Developing a general Daily Lake Evaporation Model and demonstrating its application in the state of Texas.\"\u202fWater Resources Research\u202f60.3 (2024): e2023WR036181.\n</code></pre>"},{"location":"endpoints/info-endpoints/","title":"Info Endpoints","text":"<p>The Info/Help endpoints provide information about the available data in the API, including datasets, variables, reservoirs, stations, and date ranges. These endpoints are useful for discovering what data is available and how to access it.</p>"},{"location":"endpoints/info-endpoints/#list-available-datasets","title":"List Available Datasets","text":"<p>Lists all available datasets in the API.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/info/list_datasets\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"output_format\": \"json\"  # or \"csv\"\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get available datasets\ncurl -X GET \"https://operevap.dri.edu/info/list_datasets?output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"endpoints/info-endpoints/#parameters","title":"Parameters","text":"Parameter Type Description Default output_format string Response format (json or csv) json"},{"location":"endpoints/info-endpoints/#response","title":"Response","text":"<p>Returns a list of dataset names that can be used as the <code>dataset</code> parameter in other API requests.</p>"},{"location":"endpoints/info-endpoints/#list-available-variables","title":"List Available Variables","text":"<p>Lists available variables for a specified dataset.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/info/list_variables\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"output_format\": \"json\",  # or \"csv\"\n    \"dataset\": \"nete-volume-calcs\"  # Dataset name\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get available variables for a dataset\ncurl -X GET \"https://operevap.dri.edu/info/list_variables?output_format=json&amp;dataset=nete-volume-calcs\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"endpoints/info-endpoints/#parameters_1","title":"Parameters","text":"Parameter Type Description Default output_format string Response format (json or csv) json dataset string Dataset name (rtma, res, lem, nete-volume-calcs) nete-volume-calcs"},{"location":"endpoints/info-endpoints/#response_1","title":"Response","text":"<p>Returns a list of dictionaries containing variable information. The <code>api_name</code> field in each dictionary can be used as the <code>variable</code> parameter in other API requests.</p>"},{"location":"endpoints/info-endpoints/#list-date-range","title":"List Date Range","text":"<p>Lists the minimum and maximum dates available for a dataset and variable.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/info/list_dates\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"output_format\": \"json\",  # or \"csv\"\n    \"dataset\": \"nete-volume-calcs\",\n    \"variable\": \"NetE\"\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get date range for a dataset and variable\ncurl -X GET \"https://operevap.dri.edu/info/list_dates?output_format=json&amp;dataset=nete-volume-calcs&amp;variable=NetE\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"endpoints/info-endpoints/#parameters_2","title":"Parameters","text":"Parameter Type Description Default output_format string Response format (json or csv) json dataset string Dataset name (rtma, res, lem, nete-volume-calcs) nete-volume-calcs variable string Variable name (see list_variables endpoint) NetE"},{"location":"endpoints/info-endpoints/#response_2","title":"Response","text":"<p>Returns a list containing the start date and end date for the specified dataset and variable.</p>"},{"location":"endpoints/info-endpoints/#list-reservoir-names","title":"List Reservoir Names","text":"<p>Lists available reservoir IDs (RES_NAME) that can be used in other API requests.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/info/list_RES_NAMES\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"output_format\": \"json\"  # or \"csv\"\n}\n\n# Without shapefile\nresponse = requests.post(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n\n# With shapefile (if needed)\nfiles = {\n    'shapefile_shp': open('your_shapefile.shp', 'rb'),\n    'shapefile_dbf': open('your_shapefile.dbf', 'rb'),\n    'shapefile_prj': open('your_shapefile.prj', 'rb'),\n    'shapefile_shx': open('your_shapefile.shx', 'rb')\n}\n\nresponse = requests.post(url, headers=headers, params=params, files=files)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get reservoir names without shapefile\ncurl -X POST \"https://operevap.dri.edu/info/list_RES_NAMES?output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n\n# Get reservoir names with shapefile (if needed)\ncurl -X POST \"https://operevap.dri.edu/info/list_RES_NAMES?output_format=json\" \\\n     -H \"api-key: ${API_KEY}\" \\\n     -F \"shapefile_shp=@your_shapefile.shp\" \\\n     -F \"shapefile_dbf=@your_shapefile.dbf\" \\\n     -F \"shapefile_prj=@your_shapefile.prj\" \\\n     -F \"shapefile_shx=@your_shapefile.shx\"\n</code></pre>"},{"location":"endpoints/info-endpoints/#parameters_3","title":"Parameters","text":"Parameter Type Description Default output_format string Response format (json or csv) json shapefile_shp file Optional .shp file to filter reservoirs by location shapefile_dbf file Optional .dbf file to filter reservoirs by location shapefile_prj file Optional .prj file to filter reservoirs by location shapefile_shx file Optional .shx file to filter reservoirs by location"},{"location":"endpoints/info-endpoints/#response_3","title":"Response","text":"<p>Returns a list of reservoir names that can be used as the <code>RES_NAMES</code> parameter in metadata and timeseries API requests.</p>"},{"location":"endpoints/info-endpoints/#notes","title":"Notes","text":"<p>If shapefiles are uploaded, only reservoirs within the shapefile bounds will be returned.</p>"},{"location":"endpoints/info-endpoints/#list-station-names","title":"List Station Names","text":"<p>Lists available station IDs (STA_NAME) that can be used in other API requests.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/info/list_STA_NAMES\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"output_format\": \"json\"  # or \"csv\"\n}\n\n# Without shapefile\nresponse = requests.post(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n\n# With shapefile (if needed)\nfiles = {\n    'shapefile_shp': open('your_shapefile.shp', 'rb'),\n    'shapefile_dbf': open('your_shapefile.dbf', 'rb'),\n    'shapefile_prj': open('your_shapefile.prj', 'rb'),\n    'shapefile_shx': open('your_shapefile.shx', 'rb')\n}\n\nresponse = requests.post(url, headers=headers, params=params, files=files)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get station names without shapefile\ncurl -X POST \"https://operevap.dri.edu/info/list_STA_NAMES?output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n\n# Get station names with shapefile (if needed)\ncurl -X POST \"https://operevap.dri.edu/info/list_STA_NAMES?output_format=json\" \\\n     -H \"api-key: ${API_KEY}\" \\\n     -F \"shapefile_shp=@your_shapefile.shp\" \\\n     -F \"shapefile_dbf=@your_shapefile.dbf\" \\\n     -F \"shapefile_prj=@your_shapefile.prj\" \\\n     -F \"shapefile_shx=@your_shapefile.shx\"\n</code></pre>"},{"location":"endpoints/info-endpoints/#parameters_4","title":"Parameters","text":"Parameter Type Description Default output_format string Response format (json or csv) json shapefile_shp file Optional .shp file to filter stations by location shapefile_dbf file Optional .dbf file to filter stations by location shapefile_prj file Optional .prj file to filter stations by location shapefile_shx file Optional .shx file to filter stations by location"},{"location":"endpoints/info-endpoints/#response_4","title":"Response","text":"<p>Returns a list of station names that can be used as the <code>STA_NAMES</code> parameter in metadata and timeseries API requests.</p>"},{"location":"endpoints/info-endpoints/#notes_1","title":"Notes","text":"<p>If shapefiles are uploaded, only stations within the shapefile bounds will be returned.</p>"},{"location":"endpoints/info-endpoints/#list-station-variables","title":"List Station Variables","text":"<p>Lists available variables for stations.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/info/list_stations_variables\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"output_format\": \"json\",  # or \"csv\"\n    \"STA_NAMES\": \"ALL_STATIONS\"  # or comma-separated list of station names\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get station variables\ncurl -X GET \"https://operevap.dri.edu/info/list_stations_variables?output_format=json&amp;STA_NAMES=ALL_STATIONS\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"endpoints/info-endpoints/#parameters_5","title":"Parameters","text":"Parameter Type Description Default output_format string Response format (json or csv) json STA_NAMES string One or more station names (comma-separated), or ALL_STATIONS ALL_STATIONS"},{"location":"endpoints/info-endpoints/#response_5","title":"Response","text":"<p>Returns a list of dictionaries containing variable information. The <code>api_name</code> field in each dictionary can be used as the <code>variable</code> parameter in other API requests.</p>"},{"location":"endpoints/info-endpoints/#list-station-date-range","title":"List Station Date Range","text":"<p>Lists the minimum and maximum dates available for stations in the database.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/info/list_stations_dates\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"output_format\": \"json\",  # or \"csv\"\n    \"STA_NAMES\": \"ALL_STATIONS\"  # or comma-separated list of station names\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get station date range\ncurl -X GET \"https://operevap.dri.edu/info/list_stations_dates?output_format=json&amp;STA_NAMES=ALL_STATIONS\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"endpoints/info-endpoints/#parameters_6","title":"Parameters","text":"Parameter Type Description Default output_format string Response format (json or csv) json STA_NAMES string One or more station names (comma-separated), or ALL_STATIONS ALL_STATIONS"},{"location":"endpoints/info-endpoints/#response_6","title":"Response","text":"<p>Returns a list containing the start date and end date for the specified stations.</p>"},{"location":"endpoints/metadata-endpoints/","title":"Metadata Endpoints","text":"<p>The metadata endpoints provide information about the reservoirs and stations available in the API. This metadata includes details such as location, physical characteristics, and other attributes.</p>"},{"location":"endpoints/metadata-endpoints/#reservoir-metadata","title":"Reservoir Metadata","text":"<p>Retrieves metadata for one or more reservoirs.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/metadata/reservoirs\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"RES_NAMES\": \"LAKE ALICE,LAKE ESTES\",\n    \"output_format\": \"json\"\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get reservoir metadata\ncurl -X GET \"https://operevap.dri.edu/metadata/reservoirs?RES_NAMES=LAKE%20ALICE,LAKE%20ESTES&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"endpoints/metadata-endpoints/#parameters","title":"Parameters","text":"Parameter Type Description Default output_format string Response format (json or csv) json RES_NAMES string One or more reservoir names (comma-separated) LAKE ALICE, LAKE ESTES"},{"location":"endpoints/metadata-endpoints/#response","title":"Response","text":"<p>Returns a list of dictionaries containing metadata information for the specified reservoirs. The metadata includes attributes such as:</p> <ul> <li>Geographic coordinates (latitude, longitude)</li> <li>Physical characteristics (area, depth, etc.)</li> <li>Administrative information</li> <li>Quality flags (qflag)</li> <li>And other attributes</li> </ul>"},{"location":"endpoints/metadata-endpoints/#notes","title":"Notes","text":"<p>Use the <code>/info/list_RES_NAMES</code> endpoint to discover available reservoir names.</p>"},{"location":"endpoints/metadata-endpoints/#station-metadata","title":"Station Metadata","text":"<p>Retrieves metadata for one or more stations.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/metadata/stations\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"STA_NAMES\": \"ABERDEEN_STATION,ABIQUIU DAM_STATION\",\n    \"output_format\": \"json\"\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get station metadata\ncurl -X GET \"https://operevap.dri.edu/metadata/stations?STA_NAMES=ABERDEEN_STATION,ABIQUIU%20DAM_STATION&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"endpoints/metadata-endpoints/#parameters_1","title":"Parameters","text":"Parameter Type Description Default output_format string Response format (json or csv) json STA_NAMES string One or more station names (comma-separated) ABERDEEN_STATION, ABIQUIU DAM_STATION"},{"location":"endpoints/metadata-endpoints/#response_1","title":"Response","text":"<p>Returns a list of dictionaries containing metadata information for the specified stations. The metadata includes attributes such as:</p> <ul> <li>Geographic coordinates (latitude, longitude)</li> <li>Station type</li> <li>Operational status</li> <li>Associated reservoir (if applicable)</li> <li>Quality flags (qflag)</li> <li>And other attributes</li> </ul>"},{"location":"endpoints/metadata-endpoints/#notes_1","title":"Notes","text":"<p>Use the <code>/info/list_STA_NAMES</code> endpoint to discover available station names.</p>"},{"location":"endpoints/metadata-endpoints/#using-metadata-in-other-requests","title":"Using Metadata in Other Requests","text":"<p>The metadata attributes can be included in timeseries responses by using the <code>also_return</code> parameter in timeseries requests. For example:</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/timeseries/daily/reservoirs/daterange\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"RES_NAMES\": \"LAKE ALICE\",\n    \"datasets\": \"nete-volume-calcs\",\n    \"variables\": \"NetE,E_volume\",\n    \"start_date\": \"2020-01-01\",\n    \"end_date\": \"2020-01-31\",\n    \"units\": \"metric\",\n    \"also_return\": \"qflag\",\n    \"output_format\": \"json\"\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get timeseries data with metadata attributes\ncurl -X GET \"https://operevap.dri.edu/timeseries/daily/reservoirs/daterange?RES_NAMES=LAKE%20ALICE&amp;datasets=nete-volume-calcs&amp;variables=NetE,E_volume&amp;start_date=2020-01-01&amp;end_date=2020-01-31&amp;units=metric&amp;also_return=qflag&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre> <p>This is useful for understanding the quality or context of the data returned in timeseries responses.</p>"},{"location":"endpoints/timeseries-endpoints/","title":"Timeseries Endpoints","text":"<p>The timeseries endpoints provide access to time-based data for reservoirs and stations. These endpoints are divided into two categories:</p> <ol> <li>Reservoir timeseries data</li> <li>Station timeseries data</li> </ol> <p>Each category has endpoints for retrieving data over a date range or for a single date.</p>"},{"location":"endpoints/timeseries-endpoints/#reservoir-timeseries-data-date-range","title":"Reservoir Timeseries Data (Date Range)","text":"<p>Retrieves timeseries data for one or more reservoirs over a specified date range.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/timeseries/daily/reservoirs/daterange\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"RES_NAMES\": \"LAKE ALICE\",\n    \"datasets\": \"nete-volume-calcs\",\n    \"variables\": \"NetE,E_volume\",\n    \"start_date\": \"2020-01-01\",\n    \"end_date\": \"2020-01-31\",\n    \"units\": \"metric\",\n    \"output_format\": \"json\"\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get reservoir timeseries data\ncurl -X GET \"https://operevap.dri.edu/timeseries/daily/reservoirs/daterange?RES_NAMES=LAKE%20ALICE&amp;datasets=nete-volume-calcs&amp;variables=NetE,E_volume&amp;start_date=2020-01-01&amp;end_date=2020-01-31&amp;units=metric&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"endpoints/timeseries-endpoints/#parameters","title":"Parameters","text":"Parameter Type Description Default RES_NAMES string One or more reservoir names (comma-separated) LAKE ALICE, LAKE ESTES datasets string One or more datasets (comma-separated) rtma, res, lem, nete-volume-calcs variables string One or more variables (comma-separated) pr, tmmx_c, tmmn_c, vpd_kpa, vs2m, srad, etr, eto, th, sph_kgkg, pres_pa, stage_m, area_m2, depth_m, E_hs, E_nhs, Rn, Tw, Tw0, Fetch, Lerr, NetE, E_volume, NetE_volume units string Units for returned data (english or metric) metric start_date string Start date (yyyy-mm-dd) 2020-01-01 end_date string End date (yyyy-mm-dd) 2020-01-31 also_return string Additional metadata attributes to include in response output_format string Response format (json or csv) json"},{"location":"endpoints/timeseries-endpoints/#response","title":"Response","text":"<p>Returns a list of dictionaries containing timeseries data for the specified reservoirs, datasets, variables, and date range.</p>"},{"location":"endpoints/timeseries-endpoints/#notes","title":"Notes","text":"<ul> <li>Use the <code>/info/list_RES_NAMES</code> endpoint to discover available reservoir names.</li> <li>Use the <code>/info/list_variables</code> endpoint to discover available variables for each dataset.</li> <li>Use the <code>/info/list_dates</code> endpoint to discover the valid date range for each dataset and variable.</li> </ul>"},{"location":"endpoints/timeseries-endpoints/#reservoir-timeseries-data-single-date","title":"Reservoir Timeseries Data (Single Date)","text":"<p>Retrieves timeseries data for all reservoirs on a single date.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/timeseries/daily/reservoirs/date\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"datasets\": \"nete-volume-calcs\",\n    \"variables\": \"NetE,E_volume\",\n    \"date\": \"2020-01-01\",\n    \"units\": \"metric\",\n    \"output_format\": \"json\"\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get reservoir data for a single date\ncurl -X GET \"https://operevap.dri.edu/timeseries/daily/reservoirs/date?datasets=nete-volume-calcs&amp;variables=NetE,E_volume&amp;date=2020-01-01&amp;units=metric&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"endpoints/timeseries-endpoints/#parameters_1","title":"Parameters","text":"Parameter Type Description Default datasets string One or more datasets (comma-separated) rtma, res, lem, nete-volume-calcs variables string One or more variables (comma-separated) pr, tmmx_c, tmmn_c, vpd_kpa, vs2m, srad, etr, eto, th, sph_kgkg, pres_pa, stage_m, area_m2, depth_m, E_hs, E_nhs, Rn, Tw, Tw0, Fetch, Lerr, NetE, E_volume, NetE_volume units string Units for returned data (english or metric) metric date string Date (yyyy-mm-dd) 2020-01-01 also_return string Additional metadata attributes to include in response output_format string Response format (json or csv) json"},{"location":"endpoints/timeseries-endpoints/#response_1","title":"Response","text":"<p>Returns a list of dictionaries containing timeseries data for all reservoirs for the specified datasets, variables, and date.</p>"},{"location":"endpoints/timeseries-endpoints/#station-timeseries-data-date-range","title":"Station Timeseries Data (Date Range)","text":"<p>Retrieves timeseries data for one or more stations over a specified date range.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/timeseries/stations/daterange\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"STA_NAMES\": \"LAKE MEAD_STATION\",\n    \"variables\": \"ATemp,RH\",\n    \"start_date\": \"2020-01-01\",\n    \"end_date\": \"2020-01-31\",\n    \"units\": \"metric\",\n    \"output_format\": \"json\"\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get station timeseries data\ncurl -X GET \"https://operevap.dri.edu/timeseries/stations/daterange?STA_NAMES=LAKE%20MEAD_STATION&amp;variables=ATemp,RH&amp;start_date=2020-01-01&amp;end_date=2020-01-31&amp;units=metric&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"endpoints/timeseries-endpoints/#parameters_2","title":"Parameters","text":"Parameter Type Description Default STA_NAMES string One or more station names (comma-separated) LAKE MEAD_STATION, BLW_STATION variables string One or more variables (comma-separated) ATemp, ATemp_Min, ATemp_Max, BP, BR, Ce, DO, DO_percent, EnergyT, ETo, ETr, evap_1, evap_2, evap_3, evap_4, evap_5, HSEnergyFlux, IncomingSR, inflow, SurfaceT, MO_StabilityL, NC_EnergyStored, NR, NW_AdvectedE, outflow, PH, RH, SamplingD, SkinTemp, SpecConduct, SWin, VP, VPD, WTemp, WVD, WD, WS, PofDays units string Units for returned data (english or metric) metric date string Date (yyyy-mm-dd) 2020-01-01 also_return string Additional metadata attributes to include in response output_format string Response format (json or csv) json"},{"location":"endpoints/timeseries-endpoints/#response_2","title":"Response","text":"<p>Returns a list of dictionaries containing timeseries data for all stations for the specified variables and date., WD, WS, PofDays | | units         | string | Units for returned data (english or metric)           | metric                                                                                                                                                                                                                                                                                                          | | start_date    | string | Start date (yyyy-mm-dd)                               | 2020-01-01                                                                                                                                                                                                                                                                                                      | | end_date      | string | End date (yyyy-mm-dd)                                 | 2020-01-31                                                                                                                                                                                                                                                                                                      | | also_return   | string | Additional metadata attributes to include in response |                                                                                                                                                                                                                                                                                                                 | | output_format | string | Response format (json or csv)                         | json                                                                                                                                                                                                                                                                                                            |</p>"},{"location":"endpoints/timeseries-endpoints/#response_3","title":"Response","text":"<p>Returns a list of dictionaries containing timeseries data for the specified stations, variables, and date range.</p>"},{"location":"endpoints/timeseries-endpoints/#notes_1","title":"Notes","text":"<ul> <li>Use the <code>/info/list_STA_NAMES</code> endpoint to discover available station names.</li> <li>Use the <code>/info/list_stations_variables</code> endpoint to discover available variables for stations.</li> <li>Use the <code>/info/list_stations_dates</code> endpoint to discover the valid date range for stations.</li> </ul>"},{"location":"endpoints/timeseries-endpoints/#station-timeseries-data-single-date","title":"Station Timeseries Data (Single Date)","text":"<p>Retrieves timeseries data for all stations on a single date.</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/timeseries/stations/date\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\nparams = {\n    \"variables\": \"ATemp,RH\",\n    \"date\": \"2020-01-01\",\n    \"units\": \"metric\",\n    \"output_format\": \"json\"\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get station data for a single date\ncurl -X GET \"https://operevap.dri.edu/timeseries/stations/date?variables=ATemp,RH&amp;date=2020-01-01&amp;units=metric&amp;output_format=json\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"endpoints/timeseries-endpoints/#parameters_3","title":"Parameters","text":"Parameter Type Description Default variables string One or more variables (comma-separated) ATemp, ATemp_Min, ATemp_Max, BP, BR, Ce, DO, DO_percent, EnergyT, ETo, ETr, evap_1, evap_2, evap_3, evap_4, evap_5, HSEnergyFlux, IncomingSR, inflow, SurfaceT, MO_StabilityL, NC_EnergyStored, NR, NW_AdvectedE, outflow, PH, RH, SamplingD, SkinTemp, SpecConduct, SWin, VP, VPD, WTemp, WVD"},{"location":"started/authentication/","title":"Authentication","text":""},{"location":"started/authentication/#api-key","title":"API Key","text":"<p>Access to the Reservoir Evaporation API requires an API key. This key must be included with each request to authenticate your application.</p>"},{"location":"started/authentication/#requesting-an-api-key","title":"Requesting an API Key","text":"<p>To obtain an API key, you need to make a request to the authentication endpoint:</p> PythoncURL <pre><code>import requests\n\nurl = \"https://operevap.dri.edu/auth/request_key\"\nparams = {\n    \"name\": \"John Doe\",\n    \"email\": \"john.doe@example.com\",\n    \"justification\": \"Research on reservoir evaporation for climate impact study\"\n}\n\nresponse = requests.get(url, params=params)\nprint(response.text)\n</code></pre> <pre><code># Request an API key\ncurl -X GET \"https://operevap.dri.edu/auth/request_key?name=John%20Doe&amp;email=john.doe@example.com&amp;justification=Research%20on%20reservoir%20evaporation%20for%20climate%20impact%20study\"\n</code></pre>"},{"location":"started/authentication/#parameters","title":"Parameters","text":"Parameter Type Description name string Your full name (First Last) email string A valid email address where the API key will be sent justification string A brief explanation of why you would like an API key and how you will use it"},{"location":"started/authentication/#response","title":"Response","text":"<p>After submitting your request, you should receive an email with your API key within 1 to 2 business days.</p>"},{"location":"started/authentication/#using-your-api-key","title":"Using Your API Key","text":"<p>Once you have received your API key, you must include it in the header of each API request:</p> PythoncURL <pre><code>import requests\nimport os\n\n# You can store your API key as an environment variable for security\napi_key = os.environ.get(\"OPEREVAP_API_KEY\", \"YOUR_API_KEY\")\n\nurl = \"https://operevap.dri.edu/info/list_datasets\"\nheaders = {\n    \"api-key\": api_key\n}\n\nresponse = requests.get(url, headers=headers)\ndata = response.json()\nprint(data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Make a request using your API key\ncurl -X GET \"https://operevap.dri.edu/info/list_datasets\" \\\n     -H \"api-key: ${API_KEY}\"\n</code></pre>"},{"location":"started/authentication/#example-getting-csv-output","title":"Example: Getting CSV Output","text":"PythoncURL <pre><code>import requests\nimport os\n\n# Get API key from environment variable\napi_key = os.environ.get(\"OPEREVAP_API_KEY\", \"YOUR_API_KEY\")\n\nurl = \"https://operevap.dri.edu/info/list_datasets\"\nheaders = {\n    \"api-key\": api_key\n}\nparams = {\n    \"output_format\": \"csv\"\n}\n\nresponse = requests.get(url, headers=headers, params=params)\ncsv_data = response.text\nprint(csv_data)\n\n# Optionally save to file\nwith open('datasets.csv', 'w') as f:\n    f.write(csv_data)\n</code></pre> <pre><code># Set your API key\nAPI_KEY=\"YOUR_API_KEY\"\n\n# Get data in CSV format\ncurl -X GET \"https://operevap.dri.edu/info/list_datasets?output_format=csv\" \\\n     -H \"api-key: ${API_KEY}\" \\\n     -o datasets.csv\n</code></pre>"},{"location":"started/authentication/#api-key-security","title":"API Key Security","text":"<ul> <li>Keep your API key secure and do not share it publicly.</li> <li>If you believe your API key has been compromised, request a new one.</li> <li>Do not embed your API key directly in client-side code that is accessible to users.</li> <li>Use environment variables or secure configuration files to store your API key:</li> <li>Python: Use <code>os.environ.get(\"OPEREVAP_API_KEY\")</code></li> <li>Bash/cURL: Use <code>export OPEREVAP_API_KEY=\"your-key\"</code> and reference with <code>${OPEREVAP_API_KEY}</code></li> </ul>"},{"location":"started/getting-started/","title":"Getting Started","text":"<p>This section provides an overview of how to begin using the Reservoir Evaporation API.</p>"},{"location":"started/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before using the API, you will need:</p> <ol> <li>An API key (see Authentication)</li> <li>Basic understanding of REST APIs and HTTP requests</li> <li>A tool to make HTTP requests (e.g., cURL, Postman, or programming languages like Python, JavaScript, etc.)</li> </ol>"},{"location":"started/getting-started/#base-url","title":"Base URL","text":"<p>All API requests should be made to the base URL of the API:</p> <pre><code>https://operevap.dri.edu\n</code></pre>"},{"location":"started/getting-started/#request-format","title":"Request Format","text":"<p>The API accepts request parameters as query parameters in the URL. Here's an example using Python requests:</p> <pre><code>import requests\n\nparams = {\n    \"RES_NAMES\": \"LAKE ALICE\",\n    \"start_date\": \"2020-01-01\",\n    \"end_date\": \"2020-01-31\"\n}\n\nurl = \"https://operevap.dri.edu/timeseries/daily/reservoirs/daterange\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\n\nresponse = requests.get(url, params=params, headers=headers)\ndata = response.json()\nprint(data)\n</code></pre>"},{"location":"started/getting-started/#response-format","title":"Response Format","text":"<p>The API returns data in either JSON or CSV format, which can be specified using the <code>output_format</code> parameter in your requests:</p> <ul> <li><code>output_format=json</code> (default)</li> <li><code>output_format=csv</code></li> </ul> <p>Example of requesting CSV output:</p> <pre><code>import requests\n\nparams = {\n    \"RES_NAMES\": \"LAKE ALICE\",\n    \"start_date\": \"2020-01-01\",\n    \"end_date\": \"2020-01-31\",\n    \"output_format\": \"csv\"\n}\n\nurl = \"https://operevap.dri.edu/timeseries/daily/reservoirs/daterange\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\n\nresponse = requests.get(url, params=params, headers=headers)\ncsv_data = response.text\nprint(csv_data)\n\n# Optionally save to file\nwith open('lake_alice_data.csv', 'w') as f:\n    f.write(csv_data)\n</code></pre>"},{"location":"started/getting-started/#units","title":"Units","text":"<p>For endpoints that return numerical values, you can specify whether you want the data in metric or English units using the <code>units</code> parameter:</p> <ul> <li><code>units=metric</code> (default)</li> <li><code>units=english</code></li> </ul> <p>Example using English units:</p> <pre><code>import requests\n\nparams = {\n    \"RES_NAMES\": \"LAKE ALICE\",\n    \"start_date\": \"2020-01-01\",\n    \"end_date\": \"2020-01-31\",\n    \"units\": \"english\"\n}\n\nurl = \"https://operevap.dri.edu/timeseries/daily/reservoirs/daterange\"\nheaders = {\n    \"api-key\": \"YOUR_API_KEY\"\n}\n\nresponse = requests.get(url, params=params, headers=headers)\ndata = response.json()\nprint(data)\n</code></pre>"},{"location":"started/getting-started/#api-sections","title":"API Sections","text":"<p>The API is organized into the following sections:</p> <ol> <li>Auth - Endpoints for requesting API keys</li> <li>Help/Info - Endpoints for discovering available datasets, variables, reservoirs, stations, and date ranges</li> <li>Metadata - Endpoints for retrieving metadata about reservoirs and stations</li> <li>Timeseries - Endpoints for retrieving timeseries data for reservoirs and stations</li> </ol>"},{"location":"started/getting-started/#python-example","title":"Python Example","text":"<p>A Python example for interacting with the API is available at: https://drive.google.com/drive/folders/1_JAM9JGwf40Tjo3fkx28mIP1tW5M55AO?usp=sharing</p> <p>This example can help you get started with using the API in your Python applications.</p>"},{"location":"started/getting-started/#next-steps","title":"Next Steps","text":"<ol> <li>Request an API key</li> <li>Explore the available reservoirs and stations</li> <li>Retrieve metadata for your reservoirs of interest</li> <li>Retrieve timeseries data for your analysis</li> </ol>"},{"location":"started/techstack/","title":"OperEvap Tech Stack Documentation","text":""},{"location":"started/techstack/#system-overview","title":"System Overview","text":"<p>The Desert Research Institute (DRI) operates a robust, production-grade system designed to accurately estimate reservoir evaporation. This sophisticated system leverages the Texas A&amp;M Daily Lake Evaporation Model (DLEM) as the foundational model used to estimate reservoir evaporation rates.</p> <p>The system integrates a comprehensive suite of processes including:</p> <ul> <li>Data acquisition and processing</li> <li>Advanced modeling capabilities</li> <li>Secure data storage</li> <li>Interactive data visualization</li> </ul> <p>At the heart of the system is a powerful cloud computing infrastructure comprised of Google Cloud Platform and Google Earth Engine, providing unparalleled access to vast satellite and climate datasets with advanced data analysis capabilities. The system's core database, codebase, and API are securely hosted and maintained by DRI's IT staff on a virtual machine server.</p>"},{"location":"started/techstack/#system-architecture","title":"System Architecture","text":"<pre><code>graph TD\n    %% External Data Sources\n    subgraph \"External Data Sources\"\n        A[\"Reservoir Elevation, Depth&lt;br/&gt;and Area&lt;br/&gt;Sources: USGS, CDEC, RISE,&lt;br/&gt;CPN_Hydromet, MB_Hydromet\"]\n    end\n\n    %% Earth Engine\n    subgraph \"Earth Engine\"\n        B[\"Reservoir Metadata&lt;br/&gt;(Engine Feature&lt;br/&gt;Collection)\"]\n        C[\"Meteorological Data&lt;br/&gt;(Earth Engine Image&lt;br/&gt;Collections)\"]\n        D[\"Web Application&lt;br/&gt;(Earth Engine Application)\"]\n    end\n\n    %% DRI Virtual Server\n    subgraph \"DRI Virtual Server\"\n        E[\"DLEM Model&lt;br/&gt;(Python)\"]\n        F[(\"Geodatabase&lt;br/&gt;(Postgres+PostGIS)\")]\n        G[\"API&lt;br/&gt;(FastAPI/Python)\"]\n    end\n\n    %% Connections\n    A --&gt; E\n    B --&gt; E\n    C --&gt; E\n    E --&gt; F\n    F --&gt; G\n    F --&gt; D\n\n    %% Styling\n    classDef external fill:#e3f2fd,stroke:#1976d2\n    classDef earthengine fill:#e8f5e8,stroke:#388e3c\n    classDef driserver fill:#fff3e0,stroke:#f57c00\n    classDef database fill:#fce4ec,stroke:#c2185b\n\n    class A external\n    class B,C,D earthengine\n    class E,G driserver\n    class F database</code></pre>"},{"location":"started/techstack/#system-components","title":"System Components","text":""},{"location":"started/techstack/#data-acquisition-processing","title":"Data Acquisition &amp; Processing","text":"<p>DRI manages the entire process of collecting and preparing hydrometeorological data, including:</p> <ul> <li>Unit conversion for standardized data formats</li> <li>Precomputation of relevant climate variables</li> <li>Quality control procedures to ensure data accuracy</li> </ul>"},{"location":"started/techstack/#modeling-engine-pythongoogle-earth-engine","title":"Modeling Engine (Python/Google Earth Engine)","text":"<p>The DLEM model is executed within a Python environment leveraging Google Earth Engine's cloud computing capabilities for:</p> <ul> <li>Efficient processing of large datasets</li> <li>Analysis of satellite and climate data</li> <li>Integration of historical and current data sources</li> </ul>"},{"location":"started/techstack/#geodatabase-postgresql-postgis","title":"Geodatabase (PostgreSQL + PostGIS)","text":"<p>All system data is securely stored within a PostgreSQL database utilizing the PostGIS extension:</p> <ul> <li>Reservoir metadata - Geographic and descriptive information</li> <li>Ancillary input data - Supporting datasets for calculations</li> <li>DLEM-derived evaporation estimates - Time series results</li> </ul>"},{"location":"started/techstack/#api-fastapipython","title":"API (FastAPI/Python)","text":"<p>A FastAPI-based Application Programming Interface provides programmatic access to the geodatabase:</p> <ul> <li>Base URL: https://operevap.dri.edu</li> <li>Documentation: https://operevap.launchpad.wiki/</li> <li>Enables automated data retrieval and integration</li> <li>Supports targeted data queries by reservoir, variable, and time period</li> </ul>"},{"location":"started/techstack/#web-application-earth-engine","title":"Web Application (Earth Engine)","text":"<p>An interactive web application built using the Earth Engine JavaScript API:</p> <ul> <li>URL: https://dri-apps.earthengine.app/view/bor-reservoir-evaporation</li> <li>Facilitates visualization and exploration of reservoir evaporation data</li> <li>Interactive mapping and data analysis capabilities</li> </ul>"},{"location":"started/techstack/#technical-details","title":"Technical Details","text":""},{"location":"started/techstack/#infrastructure","title":"Infrastructure","text":"<p>The system is hosted by DRI on a virtual machine server running Rocky Linux 9.5 (Blue Onyx):</p> <pre><code>Operating System: Rocky Linux 9.5\nPlatform: el9 (RHEL-compatible)\nSupport End: 2032-05-31\nVendor: Rocky Enterprise Software Foundation (RESF)\n</code></pre>"},{"location":"started/techstack/#modeling-engine-implementation","title":"Modeling Engine Implementation","text":"<p>DRI employs Python-based scripts leveraging the Earth Engine Python API that:</p> <ul> <li>Seamlessly integrate historical and current climate raster data from Earth Engine</li> <li>Combine Earth Engine data with reservoir-related data from external sources</li> <li>Utilize Earth Engine's robust processing capabilities</li> <li>Calculate area-averaged daily DLEM evaporation estimates for each reservoir</li> </ul>"},{"location":"started/techstack/#data-storage-schema","title":"Data Storage Schema","text":"<p>Time series data generated by the modeling engine is ingested into the PostgreSQL geodatabase:</p> <ul> <li>Timeseries Table: Each variable, reservoir, and time step as distinct entries</li> <li>Feature Table: Reservoir geometries and spatial information</li> <li>Feature Metadata Table: Associated descriptive metadata</li> <li>PostGIS Extension: Efficient spatial data management and queries</li> </ul> <p>All tables are accessible via API endpoint requests, enabling targeted and efficient data retrieval.</p>"},{"location":"started/techstack/#data-access-methods","title":"Data Access Methods","text":""},{"location":"started/techstack/#programmatic-access","title":"Programmatic Access","text":"<p>The FastAPI web framework serves as the API layer, providing:</p> <ul> <li>RESTful endpoints for data retrieval</li> <li>JSON-formatted responses</li> <li>Authentication and rate limiting</li> <li>Comprehensive documentation</li> </ul>"},{"location":"started/techstack/#interactive-visualization","title":"Interactive Visualization","text":"<p>The Earth Engine Web Application offers:</p> <ul> <li>Real-time data visualization</li> <li>Interactive mapping interface</li> <li>Time series plotting capabilities</li> <li>Export functionality for analysis</li> </ul>"}]}